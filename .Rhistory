hist(diffs, main = "Histogram of Simulated Rate Differences", xlab= "Differnce in Summit Rate for Business People vs Alpinists")
abline(v=obs, col="red") # observed
# Our observed rate difference is certainly on the tail of the histogram.
(sum(abs(diffs) > abs(obs))+1)/(N+1)
# Only 3.7% of our simulated rate differences are as great in magnitude as our observed difference. At at 5% significance level, we would reject the null hypothesis that there is no difference in summitting rates between business people and alpinists.
(sum(diffs < obs)+1)/(N+1)
# Contingency table of observed values with occupation type (alpinist vs business person) as rows and summit success (T/F) as columns
OBS <- table(occ$occ, occ$msuccess); OBS
# Create a version of contingency table with expected values if there is no difference in summit success by occupation
p_alp <- mean(occ$occ == "alpinist"); p_alp # proportion of observations for alpinists
p_fail <- mean(occ$msuccess == FALSE); p_fail # proportion of observations that fail to summit
N  <- nrow(occ); N # total number of observations
EXP <- N * outer(c(p_alp, 1-p_alp), c(p_fail, 1-p_fail)); EXP
# Run chi-squared test manually
chisq <- sum((OBS - EXP)^2/EXP); chisq # chi-sq statistic = 4.36
pValue <- 1 - pchisq(chisq,1); pValue # df = (2-1)*(2-1) = 1
# Confirm contingency table analysis with built-in chi-sq test
chisq.test(occ$occ, occ$msuccess, correct=F)
# first, define a function to do linear regression by projection matrix,
# and output the R-squared and regression coefficients
reg_proj <- function(x, y) {
# create A matrix, a column of 1 and columns of predictor variables
# which constitute a basis for the subspace we project onto
A <- cbind(1, x)
# Solve for the projection matrix, P.
P <- A %*% solve(t(A)%*%A) %*% t(A)
# solve for regression coefficients
coeff <- solve(t(A)%*%A) %*% t(A) %*% y
# P acting on y gives the predicted values.
yhat <- P %*% y
# calculate vector of residuals, (observed - predicted values)
res <- y - yhat
# R squared is equal to the ratio of the variance of the dependent
# variables and the variance of predicted values
Rsq <- var(yhat)/var(y)
# return R-squared and coefficients
return(list(paste0("R-Squared: ", round(Rsq[1],3)),
coeff))
}
# create 3 small dataframes holding the number of climbs and number of successful summits
# for each group
o2_used <- climbers_unique %>% filter(o2_ratio == 1) %>% select(n_climb, n_success)
o2_notused <- climbers_unique %>% filter(o2_ratio == 0) %>% select(n_climb, n_success)
o2_sometimes <- climbers_unique %>% filter(o2_ratio != 0 & o2_ratio != 1) %>% select(n_climb, n_success)
# The first regression shows that, for people who use supplemental oxygen, for every
# additional climb they go on, they can expect to summit .9 mountains. i.e. if
# someone goes on 10 climbs, we expect them to summit 9 peaks
# We have quite a large R-squared value of about .82, so we can explain about 82%
# of the variance in number of summits using only the number of climbs
reg_proj(o2_used$n_climb, o2_used$n_success)
# we see that our projection matrix approach matches R's built in function,
# and we also see that the number of climbs is statistically significant
lm.o2_used <- lm(n_success ~ n_climb, data = o2_used); summary(lm.o2_used)
# However, for those that don't use supplemental oxygen, we only expect .35
# summits per climb, or between 3 and 4 summits per every 10 climbs
# and in this model, we can only explain about 32% of the variance in
# summit success using number of climbs
reg_proj(o2_notused$n_climb, o2_notused$n_success)
# again, our values match R, and number of climbs is significant
lm.o2_notused <- lm(n_success ~ n_climb, data = o2_notused)
summary(lm.o2_notused)
# while those that use oxygen some for some fraction of their overall
# climbs, they can expect to summit about 7 mountains per 10 climbs
# here, we can explain about 79% of the variance in summit success
# using only the number of climbs
reg_proj(o2_sometimes$n_climb, o2_sometimes$n_success)
# again, our values match R, and number of climbs is significant
lm.o2_sometimes <- lm(n_success ~ n_climb, data = o2_sometimes)
summary(lm.o2_sometimes)
# create a dataframe which identifies the three groups to compare
# the slope of regression lines in a plot
o2_success_plot <- climbers_unique %>%
mutate(o2_use = case_when(o2_ratio == 1 ~ "Oxygen Always Used",
o2_ratio == 0 ~ "Oxygen Never Used",
TRUE ~ "Oxygen Sometimes Used")) %>%
select(climb_id, n_climb, n_success, o2_use, o2_ratio)
# binning oxygen use into 3 distinct groups lends itself nicely to a
# visual comparison of the regression lines. Here we can see the
# disparity in the slope between the three groups
ggplot(data = o2_success_plot, aes(x = n_climb, y = n_success, color = o2_use, shape = o2_use)) +
geom_point( size = 2) +
geom_smooth(method = lm, se = FALSE, fullrange = TRUE, size = .8) +
scale_color_manual(values = c("cadetblue4", "firebrick4", "azure4")) +
scale_shape_manual(values = c(19, 19, 24)) +
labs (
title = "Summit Success by Oxygen Use Over Multiple Climbs",
subtitle = "(All Peaks)",
x = "Number of Climbs",
y = "Number of Succesful Summits"
) +
theme_classic() +
theme(
plot.title = element_text(size = 20, face = "bold", hjust = .5),
plot.subtitle = element_text(size = 14, hjust = .5),
axis.text.x = element_text(size = 12, color = "black"),
axis.text.y = element_text(size = 12, color = "black"),
axis.title = element_text(size = 14, color = "black"),
legend.title = element_blank()
)
# create matrix of predictor variables, convert "sex" into a numeric dummy,
# and filter out ages entered as 0
o2_fullreg <- climbers_unique %>%
mutate(sex = ifelse(sex == "M", 1, 0)) %>%
filter(calcage > 0 )
A1 <- o2_fullreg %>% select(o2_ratio, n_climb, died, calcage, sex) %>% as.matrix()
# using the projection matrix approach
reg_proj(A1, o2_fullreg$n_success)
# and we see it matches the R function, and that all of our variables enter
# significantly in the model
lm.success <- lm(n_success ~ o2_ratio + n_climb + died + calcage + sex, data = o2_fullreg); summary(lm.success)
# extract variables for success and use of oxygen
summited <- climbers$msuccess
mo2used <- climbers$mo2used
plot(mo2used, summited,
main = "Oxygen Use Impact on Odds of Summiting",
xlab = "Oxygen Used (1 = yes, 0 = no)",
ylab = "Summited (1 = yes, 0 = no)")
# use log likelihood function to estimate the parameters
MLL<- function(alpha, beta) {
-sum( log( exp(alpha+beta*mo2used)/(1+exp(alpha+beta*mo2used)) )*summited
+ log(1/(1+exp(alpha+beta*mo2used)))*(1-summited) )
}
# fit the model with initial guess of alpha = 0, beta = 0
results<-mle(MLL, start = list(alpha = 0, beta = 0))
results@coef
# plot the regression curve
curve( exp(results@coef[1]+results@coef[2]*x)/ (1+exp(results@coef[1]+results@coef[2]*x)),col = "blue", add=TRUE)
# we see that our estimated coefficients match those in R's logit function,
# and that oxygen use is statistically significant
summary(glm(msuccess~mo2used, family = "binomial", data = climbers))
# to get a more intuitive understanding of using oxygen on summiting, we
# take the exponential of the coefficient on oxygen use to get the odds ratio
exp(results@coef[2])
# though of course, use of oxygen is not the only thing that impacts the odds
# of summiting a mountain. So we build a larger model to control for other factors
# such as the age of the climber, whether they climb in the spring or not, sex,
# and whether or not the climber is hired help
logit_reg <- climbers %>% filter(!is.na(calcage))
summited <- logit_reg$msuccess %>% as.numeric()
mo2used <- logit_reg$mo2used %>% as.numeric()
main_season <- logit_reg$main_season
sex <- logit_reg %>% mutate(sex = ifelse(sex == "M", 1, 0)) %>% pull(sex)
hired <- logit_reg$hired %>% as.numeric()
age <- logit_reg$calcage
# we start by using an expanded version of the log likelihood method to estimate more parameters
MLL<- function(alpha, beta1, beta2, beta3, beta4, beta5) {
-sum( log( exp(alpha + beta1*mo2used + beta2*main_season + beta3*sex + beta4*hired + beta5*age)/
(1+exp(alpha + beta1*mo2used + beta2*main_season + beta3*sex + beta4*hired + beta5*age)) )*summited
+ log(1/(1+exp(alpha + beta1*mo2used + beta2*main_season + beta3*sex + beta4*hired + beta5*age)))*(1-summited) )
}
# fit the model with initial guess of alpha = 0, beta's = 0
results<-mle(MLL, start = list(alpha = 0, beta1 = 0, beta2 = 0, beta3 = 0 , beta4 = 0,  beta5 = 0))
results@coef
# checking against R's built in function once again shows close agreement, and
# also significance for every variable except sex
logit.o2_full <- glm(msuccess ~ mo2used + main_season + sex + hired  + calcage, family = "binomial", data = logit_reg); summary(logit.o2_full)
exp(coef(logit.o2_full))
# Load dataset with hourly weather readings from Everest basecamp starting Nov 1, 2019 to Jun 30, 2021
basecamp <- read_csv("Base_Camp_20210630.csv")
# Reformat and rename variables
basecamp <- basecamp %>%
mutate(time = as.POSIXct(TIMESTAMP, format = "%m/%d/%Y %H:%M")) %>% # converts character to date-time
rename(humid = RH, # rename columns to more descriptive names
temp = T_HMP,
pressure = PRESS,
precip = PRECIP)
# Create new variables from our date-time variable
basecamp <- basecamp %>%
mutate(date = format(time, "%m/%d/%Y"),
month = format(time, "%m"),
day = format(time, "%d"),
year = format(time, "%Y"),
hr = format(time, "%H"),
time_24 = format(time, "%H:%M"))
setwd("C:/Courses/M23c-2022/Scripts/Final/Otzis_Adventures")
# Load dataset with hourly weather readings from Everest basecamp starting Nov 1, 2019 to Jun 30, 2021
basecamp <- read_csv("Base_Camp_20210630.csv")
# Reformat and rename variables
basecamp <- basecamp %>%
mutate(time = as.POSIXct(TIMESTAMP, format = "%m/%d/%Y %H:%M")) %>% # converts character to date-time
rename(humid = RH, # rename columns to more descriptive names
temp = T_HMP,
pressure = PRESS,
precip = PRECIP)
# Create new variables from our date-time variable
basecamp <- basecamp %>%
mutate(date = format(time, "%m/%d/%Y"),
month = format(time, "%m"),
day = format(time, "%d"),
year = format(time, "%Y"),
hr = format(time, "%H"),
time_24 = format(time, "%H:%M"))
# Construct a dataset with just the spring weather observations (2020 & 2021)
spring_weather <- basecamp %>%
filter(month == "03" | month == "04" | month == "05") %>% # the most popular season for expeditions is spring: March - May
group_by(date) %>%
mutate(total_precip = sum(precip), # total precipitation over the course of a day
min_temp = min(temp)) %>% # minimum temperature over the course of a day
ungroup %>%
mutate(rainy_day = as.numeric(total_precip > 0), # rainy day if have any precipitation
high_humidity = as.numeric(humid > mean(humid))) # high humidity day if above average humidity
# Let's see if there are other weather variables that might be easier to measure/predict that help us predict barometric pressure
cor(spring_weather$temp, spring_weather$pressure) # Correlation coefficient is almost 0.58 - that's pretty strong. Temperature might serve as a good predictor for barometric pressure.
cor(spring_weather$humid, spring_weather$pressure)
cor(spring_weather$high_humidity, spring_weather$pressure) # Both of the correlation coefficients for humidity-related variables are under 0.20, might not be the best to start there with creating a model.
m1 <- rep(1, length(spring_weather)) # vector of 1s
m2 <- spring_weather$temp # temperature
m3 <- spring_weather$pressure # pressure
A <- cbind(m1, m2) # vector of 1s, temperature
A_t <- t(A) # transpose of A
B <- A_t%*%A # A_t %*% A
B_inv <- solve(B); B_inv # invert B
coeff <- B_inv%*%A_t%*%m3; coeff # coefficients come from B_inv %*% A_t %*% pressure
b <- coeff[1,1]; b # intercept: 534.027. Interpretation: If it is 0 degrees Celsius, we expect the barometric pressure to be 534.027 hPa
a <- coeff[2,1]; a # slope: 0.347086. Interpretation: For every 1 degree increase in temperature, we expect the barometric pressure to increase by 0.37 hPa.
# Manual calculation of R^2
pred_pressure <- b + a*spring_weather$temp
spring_weather$resid_pressure = spring_weather$pressure - pred_pressure
var_pred <- var(pred_pressure)
var_obs <- var(spring_weather$pressure)
R2 <- var_pred/var_obs; R2 # We can explain about 34% of the variation in pressure by temperature.
N <- nrow(spring_weather)
Adj_R2 <- 1 - ( (1-R2) * (N-1) / (N-1-1) ); Adj_R2 # 1 independent variable; Adjusted for number of parameters, we still can explain about 34% of the variation in pressure by temperature.
# Let's check this against the built-in function
summary(lm(pressure ~ temp, data = spring_weather)) # Our coefficients and R-squared values match those we manually calculated!
# Let's plot the data and the regression line we calculated
ggplot(data = spring_weather, aes(x = temp, y = pressure)) +
geom_point() +
labs(title = "Spring Temperatures and Barometric Pressure at Everest Basecamp",
x = "Temperature (degrees C)",
y = "Barometric Pressure (hPa)") +
geom_abline(intercept = b, slope = a, color="red", size=1.5)
# Let's double check that a linear relationship is appropriate by examining the residual plot.
ggplot(data = spring_weather, aes(x = temp, y=resid_pressure)) +
geom_point() +
labs(title= "Residual Plot for Pressure ~ Temperature",
x= "Temperature (degrees C)",
y = "Residual") +
geom_hline(yintercept = 0, color ="blue")
# Both of our humidity-related variables had much weaker correlation to pressure. But, we can test adding one of them to our model.
# First, let's check that we would be adding a second predictor that is not highly correlated with temperature, that would pose the issue of multi-colinearity.
cor(spring_weather$humid, spring_weather$temp)
cor(spring_weather$high_humidity, spring_weather$temp) # Neither humidity measure appears to be correlated with temperature.
# Because we are using a factor variable, it is easy to add a visualization of a 3rd variable. We can start here to see what results we might expect by following the same steps as above.
ggplot(data = spring_weather, aes(x = temp, y = pressure, color = as.factor(high_humidity))) +
geom_point() +
labs(title = "Spring Temperatures and Barometric Pressure at Everest Basecamp",
x = "Temperature (degrees C)",
y = "Barometric Pressure (hPa)",
color = "High Humidity")
# Using projection matrix,
m1 <- rep(1, length(spring_weather))
m2 <- spring_weather$temp
m3 <- spring_weather$high_humidity
m4 <- spring_weather$pressure
A <- cbind(m1, m2, m3)
A_t <- t(A)
B <- A_t%*%A
B_inv <- solve(B); B_inv #
coeff <- B_inv%*%A_t%*%m4; coeff
b <- coeff[1,1]; b # Intercept: 533.6646. Interpretation: If it is 0 degrees Celsius and non-high humidity, we expect the barometric pressure to be 533.6646 hPa.
a1 <- coeff[2,1]; a1 # Coefficient on temperature: 0.34624 . Interpretation: For every 1 degree increase in temperature, holding high humidity status constant, we expect the barometric pressure to increase by 0.35 hPa.
a2 <- coeff[3,1]; a2 # Coefficient on high humidity: 0.6470104. Interpretation: Holding temperature constant, we expect high-humidity observations to have barometric pressure 0.65 hPa higher than low-humidity observations.
# Manual calculation of R^2
pred_pressure <- b + a1*spring_weather$temp + a2*spring_weather$high_humidity
spring_weather$resid_pressure = spring_weather$pressure - pred_pressure
var_pred <- var(pred_pressure)
var_obs <- var(spring_weather$pressure)
R2 <- var_pred/var_obs; R2 # We can explain about 35% of the variation in pressure by temperature and high humidity status. This is only 1% higher than if we use only temperature, indicating that we probably shouldn't be using high humidity as a predictor.
N <- nrow(spring_weather)
Adj_R2 <- 1 - ( (1-R2) * (N-1) / (N-2-1) ); Adj_R2 # 2 independent variable; Adjusted for number of parameters, we still can explain about 35% of the variation in pressure by temperature.
# Let's check this against the built-in function
summary(lm(pressure ~ temp + high_humidity, data = spring_weather)) # All our coefficients and R-square calculations match the built-in function.
# Since temperature can serve as a predictor for barometric pressure, let's see if we can model temperatures over the spring season.
temp_2020 <- spring_weather[1:2207,]$temp # temperatures in 2020
temp_2021 <- spring_weather[2208:4414,]$temp # temperatures in 2021
avg_temp <- (temp_2020 + temp_2021)/2 # take the average of our 2 years of observations
month <- spring_weather[1:2207,]$month
day <- spring_weather[1:2207,]$day
hr <- spring_weather[1:2207,]$hr
avg_spring_temp <- cbind(month, day, hr, avg_temp) # dataframe with avg temp at date and time (i.e. avg temp on April 1 at 06:00)
nhours <- nrow(avg_spring_temp); nhours # 2207
plot(1:nhours, avg_temp, type = "l", main = "Average temperatures in Spring", xlab="Hour index", ylab="Average temperature (in degrees C)") # line plot of temperature over the spring
# FUNCTIONS
# Cosine and sine functions with m periods in 2207 hours
myCos <- function(m) cos((1:nhours)*m*2*pi/nhours)
mySin <- function(m) sin((1:nhours)*m*2*pi/nhours)
# Fourier coefficient a_m
coeffA <- function(m){
sum(avg_temp*2*myCos(m)/nhours)
}
# Fourier coefficient b_m
coeffB <- function(m){
sum(avg_temp*2*mySin(m)/nhours)
}
# Takes a set number of coefficients and overlays the Fourier plot
plot_Fourier <- function(ncoeff) {
FourierA <- sapply(1:ncoeff,coeffA)
FourierB <- sapply(1:ncoeff,coeffB)
#Find the amplitude of the sum of the cosine and sine terms
Fourier <- sqrt(FourierA^2+FourierB^2)
Fourier
recon <- mean(avg_temp)
for (m in 1:ncoeff) {
recon <- recon + FourierA[m]*myCos(m)+FourierB[m]*mySin(m)
}
plot(1:nhours, avg_temp, type = "l", main = "Average temperatures in Spring", xlab="Hour index", ylab="Average temperature (in degrees C)")
points(1:nhours,recon, type = "l", col = "red",lwd = 2)
}
# Evaluate the first 100 coefficients to see which ones might be the largest
#Evaluate ncoeff coefficients
ncoeff <- 100
FourierA <- sapply(1:ncoeff,coeffA)
FourierB <- sapply(1:ncoeff,coeffB)
#Find the amplitude of the sum of the cosine and sine terms
Fourier <- sqrt(FourierA^2+FourierB^2)
Fourier
# ncoeff = 1; The first Fourier coefficient is the largest
plot_Fourier(1)
# ncoeff = 3; The first 3 Fourier coefficients are all large, and this would correspond to the 3 months of data.
plot_Fourier(3)
# ncoeff = 13; Even though the 13th Fourier coefficient isn't large, there are approximately 13 weeks in the season so this would pick up weekly trends.
plot_Fourier(13)
# ncoeff = 92; The 92nd coefficient is large, reflecting the fact that there are 92 days of data and that daily variation is important.
plot_Fourier(92)
# ncoeff = 200; Let's see if 200 coefficients gets us a perfect replication.
plot_Fourier(200)
min_daily_temp <- spring_weather %>%
filter(hr == "01") %>%
select(min_temp)
min_daily_temp <- min_daily_temp$min_temp
mu <- mean(min_daily_temp); mu # population mean
sigma <- sd(min_daily_temp); sigma # population standard deviation
n <- 30 # sample size
samp <- sample(min_daily_temp, n, replace =FALSE)
X_bar <- mean(samp); X_bar # sample mean
L <- X_bar + qt(0.025, df = n-1) * sigma/sqrt(n); L
U <- X_bar + qt(0.975, df = n-1) * sigma/sqrt(n); U
# Let's repeat this a large number of times and check that we are capturing the true mean in 95% of the confidence intervals we construct.
counter <- 0 # counter to record every time the CI contains the population mean
counter_mu_above <- 0 # counter to record every time the CI is below the population mean (U < mu)
counter_mu_below <- 0 # counter to record every time the CI is over the population mean (mu < L)
N <-10000
for (i in 1:N) {
samp <- sample(min_daily_temp, n, replace =FALSE)
L <- mean(samp) + qt(0.025, n-1) * sigma/sqrt(n) #usually less than the true mean
U <- mean(samp) + qt(0.975, n-1) * sigma/sqrt(n) #usually greater than the true mean
if (L < mu && U > mu) counter <- counter + 1 #count +1 if we were correct
if (U < mu) counter_mu_above <- counter_mu_above + 1
if (L > mu) counter_mu_below <- counter_mu_below + 1
}
counter; counter_mu_above; counter_mu_below
# We are actually getting about 97.5% of our confidence intervals containing the population mean. It also appears that the confidence intervals we are constructing are more likely to overestimate than underestimate the true mean (more observations of counter_mu_below).
hist(min_daily_temp, main = "Histogram", xlab = "Minimum Daily Temperature", breaks = "FD")
abline(v = mu, col="red")
# Select subset of data that will be used for power analyses
valid_expeditions <- df %>%
group_by(expid) %>%
filter(calcage > 0) %>% # will consider using age as a covariate; assuming age 0 is error
filter(peakid == 'EVER') %>% # want to focus just on Everest for potential study
summarise(count = n(), num_deaths = sum(death)) %>%
filter(count > 10) %>% # need sufficient obs within each expedition for multilevel model
filter(num_deaths > 0) # need expeditions that experienced at least one fatality, for
# classical approach to power
length(valid_expeditions$expid) # these criteria leave us with 34 expeditions total
model_data <- df %>%
filter(expid %in% valid_expeditions$expid) %>%
select(expid, membid, peakid, mo2used, calcage, death) # final model data
# Data aggregated to expedition level, with indicator for any deaths and
# continuous variable that is proportion of expedition that used supplemental
# oxygen
model_data_logit <- df %>%
filter(peakid == 'EVER') %>%
group_by(expid) %>%
summarise(prop_mo2used = mean(mo2used),
any_death = max(death))
# Logit model
logit_model <- glm(any_death ~ prop_mo2used,
family = binomial(link = 'logit'),
data = model_data_logit)
# Now we can plot margins using fit logit model
base::plot(model_data_logit$prop_mo2used,
model_data_logit$any_death,
type = 'p',
main = 'Mortaility as a Function of the Rate of Oxygen Use',
xlab = 'Proportion of Expedition that Used Supplemental Oxygen',
ylab = 'Probability of Any Death on Expedition')
logit_coefs <- as.vector(logit_model$coefficients); logit_coefs
inv_logit_transform <- function(x,
a = logit_coefs[1],
b = logit_coefs[2]) {
exp(a + b*x)/(1 + exp(a + b*x))
}
curve(inv_logit_transform(x), add = TRUE, col = 'red')
# Fitting with random slopes and intercepts for each expedition
m1 <- lmer(death ~ mo2used + (1 + mo2used | expid), data = model_data); m1
# Extracting the random component variances and covariances
m1_random_effects <- as.data.frame(VarCorr(m1)); m1_random_effects
# Extracting the fixed effects estimates
m1_fixed_effects <- fixef(m1); m1_fixed_effects
true_effect <- as.vector(m1_fixed_effects['mo2usedTRUE']); true_effect
# This approach makes use of the fact that to achieve 80% power, you need a
# true effect that is 2.8 standard deviations away from a comparison point
# (and of course making the common assumption that treatment estimates are
# distributed asymptotically normal around that true effect). Our comparison
# point will be 0 -- as in the null hypothesis that supplemental oxygen use
# does nothing to reduce or increase mortality.
pnorm(1.96, 2.8, 1, lower.tail = FALSE) # assuming a std. dev. of 1, we see 2.8 gives 80% power
# First, we need to calculate some summary metrics across the 34 expeditions
t <- model_data %>%
group_by(expid) %>%
summarise(count_climbers = n(), prop_mo2used = mean(mo2used))
mean_climbers <- mean(t$count_climbers); mean_climbers # ~26 climbers/expedition on avg.
x_bar <- mean(t$prop_mo2used); x_bar # ~60% of expedition members typically used sup. oxygen
assumed_num_using_mo2 <- ceiling(x_bar*mean_climbers); assumed_num_using_mo2 # assumed number who typically use oxygen
x <- c(rep(1, assumed_num_using_mo2), rep(0, 26 - assumed_num_using_mo2)); x # assumed expedition vector of sup. oxy. use
sigma_y <- m1_random_effects[4,'sdcor']; sigma_y # estimated residual variance from m1
sigma_beta_estimation <- 1/sqrt(sum((x - x_bar)^2))*sigma_y; sigma_beta_estimation # estimation variance for typical Beta_j
# Check our estimation variance against distribution of std. errors from actually
# fitting a linear probability model for each expedition
sorted_expid <- sort(unique(model_data$expid))
n <- length(sorted_expid)
l <- list()
for (i in 1:n) {
l[[i]] <- lm(death ~ mo2used,
data = subset(model_data, expid == sorted_expid[i]))
}
mo2used_se <- unlist(lapply(l, function(m) sqrt(vcov(m)['mo2usedTRUE','mo2usedTRUE'])))
hist(mo2used_se, prob = TRUE,
main = 'Distribution of Standard Errors for Oxygen Use',
xlab = 'Std. Error') # we see that our "typical" estimate of estimation variance aligns with these results
abline(v = sigma_beta_estimation, col = 'red') # our "typical estimate" plotted on the histogram
# Now we can calculate the variance for all Beta_j's (coefficient on oxygen use),
# incorporating variance in both the true Beta_j's and variance due to estimation
sigma_beta <- m1_random_effects[2,'sdcor']; sigma_beta # variance across true Beta_j's, estimated from m1
sigma_beta_j <- sqrt(sigma_beta^2 + sigma_beta_estimation^2); sigma_beta_j # estimated total variance across Beta_j's
# Finally, we can plug our results into the following formula. This formula is the
# result of solving the standard error formula for a treatment estimate in a group-
# randomized model for number of groups randomized:
#           J = (2*2.8*variance/(true effect size))^2
J_classical <- (2*2.8*sigma_beta_j/true_effect)^2; J_classical
# Function to create simulated data sets, taking arguments of
# J expeditions, each with K climbers; note that J should be even
make_fake <- function(J, K) {
using_oxygen <- rep(c(rep(1, ceiling(K*x_bar)), rep(0, K - ceiling(K*x_bar))), J) # assuming in control group that x_bar of each expedition using oxygen
expedition <- rep(1:J, each = K) # expedition ids
treatment <- sample(rep(0:1, J/2))
treatment1 <- treatment[expedition]
mo2used <- integer(J*K)
for (j in 1:J) {
if (treatment[j] == 1) {
mo2used[1:K + (j - 1)*K] <- rep(1, K)
} else {
mo2used[1:K + (j - 1)*K] <- using_oxygen[1:K + (j - 1)*K]
}
}
# Hyperparameters for population of expeditions
gamma.0.true <- as.vector(m1_fixed_effects['(Intercept)'])
gamma.1.true <- true_effect
sigma.a.true <- m1_random_effects[1,'sdcor']
mu.b.true <- as.vector(m1_fixed_effects['mo2usedTRUE'])
sigma.b.true <- sigma_beta
sigma.y.true <- sigma_y
# Expedition-level parameter draws
# note: claiming treatment of requiring oxygen affects intercept for treated expeditions, because oxygen use becomes constant, can't estimate separate beta
a.true <- rnorm(J, gamma.0.true + gamma.1.true*treatment, sigma.a.true)
b.true <- rnorm(J, mu.b.true, sigma.b.true)
for (i in 1:length(a.true)) { # negative intercepts need to be truncated at 0, in order to parameterize Binomial
if (a.true[i] < 0) {
a.true[i] <- 0
}
}
# Generating climber-level data using expedition-level parameter draws
y <- numeric(J*K)
for (j in 1:J) {
if (treatment[j] == 1) {
draws <- rbinom(K, 1, a.true[j])
} else {
p_vec <- a.true[j] + b.true[j]*using_oxygen[1:K + (j - 1)*K]
for (i in 1:length(p_vec)) {
if (p_vec[i] < 0) {
p_vec[i] <- 0
}
}
draws <- rbinom(K, 1, p_vec)
}
y[1:K + (j - 1)*K] <- draws
}
return(data.frame(y, mo2used, expedition, treatment1))
}
# Function that calls data-simulation function and then estimates treatment
# effects, returning the proportion of iterations that yielded a statistically
# significant result (i.e., power)
powe.R <- function(J, K, n_sims = 1000) {
signif <- rep(NA, n_sims)
for (s in 1:n_sims) {
fake <- make_fake(J, K)
lme.power <- lmer(y ~ mo2used + treatment1 + (1 + mo2used | expedition) , data = fake) # note that treatment1 here is just affecting intercept
theta.hat <- fixef(lme.power)['treatment1']
theta.se <- sqrt(vcov(lme.power)['treatment1', 'treatment1'])
signif[s] <- abs(theta.hat/theta.se) > 2
}
power <- mean(signif)
return(power)
}
# Generating power as a function of expeditions randomized, assuming 27 climbers per expedition,
# which was roughly the mean for Everest climbs:
# J_vec <- seq(50, 1200, by = 50); J_vec #! warning: full code here takes 2 hrs to run; will just show a test case below
# power_vec <- sapply(J_vec, function(x) powe.R(J = x, K = 27))
J_vec <- 80 # this was our result from classical approach; simulation says this is very inadequate sample size
power_vec <- sapply(J_vec, function(x) powe.R(J = x, K = 27))
# ggplot(data.frame(J_vec, power_vec)) +
